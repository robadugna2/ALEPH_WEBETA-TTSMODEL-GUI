{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rly9FYMoLc8j"
      },
      "outputs": [],
      "source": [
        "# System-level installations\n",
        "!sudo apt-get update && sudo apt-get install -y espeak-ng ffmpeg\n",
        "\n",
        "# Python package installations\n",
        "!pip install -q numpy==1.21 torch torchaudio\n",
        "!pip install -q TTS gradio librosa soundfile pydub jiwer nltk pyworld pynini\n",
        "!pip install -q phonemizer unidecode inflect eng_to_ipa num2words pyloudnorm pysoundfile\n",
        "!pip install -q resampy matplotlib tensorboard tqdm coqpit fsspec gdown\n",
        "!pip install -q google-auth-oauthlib google-auth-httplib2 google-api-python-client\n",
        "!pip install -q requests aiohttp\n",
        "!pip install -q transformers datasets\n",
        "!pip install -q pyyaml\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q pypinyin\n",
        "!pip install -q jieba\n",
        "!pip install -q gruut\n",
        "!pip install -q g2p_en\n",
        "!pip install -q epitran\n",
        "!pip install -q panphon\n",
        "!pip install -q pycnnum\n",
        "!pip install -q mecab-python3\n",
        "!pip install -q unidic-lite\n",
        "!pip install -q jaconv\n",
        "!pip install -q ko_pron\n",
        "!pip install -q indic_transliteration\n",
        "!pip install -q aksharamukha\n",
        "!pip install -q bnnumerizer\n",
        "!pip install -q bnunicodenormalizer\n",
        "!pip install -q cython\n",
        "!pip install -q scipy\n",
        "!pip install pyloudnorm\n",
        "\n",
        "# Install Whisper\n",
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "\n",
        "# Install Coqui TTS from source\n",
        "!git clone https://github.com/coqui-ai/TTS\n",
        "%cd TTS\n",
        "!pip install -e .[all,dev,notebooks]\n",
        "%cd ..\n",
        "\n",
        "print(\"All installations complete. You may need to restart your runtime once for all changes to take effect.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJRdtpbkVylX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import gradio as gr\n",
        "from pathlib import Path\n",
        "import soundfile as sf\n",
        "import subprocess\n",
        "import whisper\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "import sys\n",
        "import site\n",
        "site.main()\n",
        "sys.path.extend(site.getsitepackages())\n",
        "\n",
        "# Update these imports to match the current TTS library structure\n",
        "from TTS.config.shared_configs import BaseDatasetConfig\n",
        "from TTS.tts.configs.vits_config import VitsConfig\n",
        "from TTS.tts.datasets import load_tts_samples\n",
        "from TTS.tts.models.vits import Vits\n",
        "from TTS.utils.audio import AudioProcessor\n",
        "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
        "from TTS.bin.compute_embeddings import compute_embeddings\n",
        "import torch\n",
        "import torchaudio\n",
        "import librosa\n",
        "import nltk\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from jiwer import wer\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Constants\n",
        "SPEAKER_ENCODER_CHECKPOINT_PATH = \"https://github.com/coqui-ai/TTS/releases/download/speaker_encoder_model/model_se.pth.tar\"\n",
        "SPEAKER_ENCODER_CONFIG_PATH = \"https://github.com/coqui-ai/TTS/releases/download/speaker_encoder_model/config_se.json\"\n",
        "\n",
        "def preprocess_audio(file_path, normalize=True, noise_reduce=True, trim_silence=True):\n",
        "    y, sr = librosa.load(file_path, sr=None)\n",
        "    if normalize:\n",
        "        y = librosa.util.normalize(y)\n",
        "    if noise_reduce:\n",
        "        y = librosa.effects.remix(y, segments=librosa.effects.split(y, top_db=20))\n",
        "    if trim_silence:\n",
        "        y, _ = librosa.effects.trim(y, top_db=20)\n",
        "    return y, sr\n",
        "\n",
        "def clean_transcript(text, cleaner='amharic_cleaners'):\n",
        "    if cleaner == 'amharic_cleaners':\n",
        "        return amharic_cleaners(text)\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "def augment_audio(y, sr, pitch_shift=0, time_stretch=1.0, add_noise=False):\n",
        "    if pitch_shift != 0:\n",
        "        y = librosa.effects.pitch_shift(y, sr=sr, n_steps=pitch_shift)\n",
        "    if time_stretch != 1.0:\n",
        "        y = librosa.effects.time_stretch(y, rate=time_stretch)\n",
        "    if add_noise:\n",
        "        noise = np.random.randn(len(y))\n",
        "        y = y + 0.005 * noise\n",
        "    return y\n",
        "\n",
        "def augment_text(text):\n",
        "    # Implement text augmentation techniques here\n",
        "    # For example, you can use nltk for synonym replacement\n",
        "    return text\n",
        "\n",
        "def process_audio(upload_dir, subfolder, run_denoise, run_splits, use_audio_filter, normalize_audio):\n",
        "    orig_wavs = os.path.join(upload_dir, subfolder, \"22k_1ch\")\n",
        "    os.makedirs(orig_wavs, exist_ok=True)\n",
        "\n",
        "    # Convert audio files to 22kHz mono WAV\n",
        "    for ext in ['mp3', 'ogg', 'wav']:\n",
        "        files = glob.glob(os.path.join(upload_dir, subfolder, f'*.{ext}'))\n",
        "        for file in files:\n",
        "            output_file = os.path.join(orig_wavs, os.path.splitext(os.path.basename(file))[0] + '.wav')\n",
        "            subprocess.run(['ffmpeg', '-i', file, '-acodec', 'pcm_s16le', '-ar', '22050', '-ac', '1', output_file])\n",
        "\n",
        "    if run_denoise == \"True\":\n",
        "        # Implement denoising logic here\n",
        "        denoised_dir = os.path.join(upload_dir, subfolder, \"denoised\")\n",
        "        os.makedirs(denoised_dir, exist_ok=True)\n",
        "        for wav_file in glob.glob(os.path.join(orig_wavs, \"*.wav\")):\n",
        "            output_file = os.path.join(denoised_dir, os.path.basename(wav_file))\n",
        "            subprocess.run(['ffmpeg', '-i', wav_file, '-af', 'afftdn=nf=-25', output_file])\n",
        "\n",
        "    if run_splits == \"True\":\n",
        "        # Implement splitting logic here\n",
        "        split_dir = os.path.join(upload_dir, subfolder, \"splits\")\n",
        "        os.makedirs(split_dir, exist_ok=True)\n",
        "        for wav_file in glob.glob(os.path.join(orig_wavs, \"*.wav\")):\n",
        "            output_prefix = os.path.join(split_dir, os.path.splitext(os.path.basename(wav_file))[0])\n",
        "            subprocess.run(['ffmpeg', '-i', wav_file, '-f', 'segment', '-segment_time', '10', '-c', 'copy', f'{output_prefix}_%03d.wav'])\n",
        "\n",
        "    if use_audio_filter == \"True\":\n",
        "        # Implement audio filtering logic here\n",
        "        filtered_dir = os.path.join(upload_dir, subfolder, \"filtered\")\n",
        "        os.makedirs(filtered_dir, exist_ok=True)\n",
        "        for wav_file in glob.glob(os.path.join(orig_wavs, \"*.wav\")):\n",
        "            output_file = os.path.join(filtered_dir, os.path.basename(wav_file))\n",
        "            subprocess.run(['ffmpeg', '-i', wav_file, '-af', 'highpass=f=200,lowpass=f=3000', output_file])\n",
        "\n",
        "    if normalize_audio == \"True\":\n",
        "        # Implement normalization logic here\n",
        "        normalized_dir = os.path.join(upload_dir, subfolder, \"normalized\")\n",
        "        os.makedirs(normalized_dir, exist_ok=True)\n",
        "        for wav_file in glob.glob(os.path.join(orig_wavs, \"*.wav\")):\n",
        "            data, rate = sf.read(wav_file)\n",
        "            meter = pyln.Meter(rate)\n",
        "            loudness = meter.integrated_loudness(data)\n",
        "            loudness_normalized_audio = pyln.normalize.loudness(data, loudness, -23.0)\n",
        "            output_file = os.path.join(normalized_dir, os.path.basename(wav_file))\n",
        "            sf.write(output_file, loudness_normalized_audio, rate)\n",
        "\n",
        "def transcribe_audio(ds_name, newspeakername, whisper_model, whisper_lang):\n",
        "    model = whisper.load_model(whisper_model)\n",
        "    wavs = f'/content/drive/MyDrive/{ds_name}/wav48_silence_trimmed/{newspeakername}'\n",
        "    txt_dir = f'/content/drive/MyDrive/{ds_name}/txt/{newspeakername}/'\n",
        "    os.makedirs(txt_dir, exist_ok=True)\n",
        "\n",
        "    for filepath in glob.glob(os.path.join(wavs, '*.flac')):\n",
        "        result = model.transcribe(filepath, language=whisper_lang)\n",
        "        output = result[\"text\"].strip()\n",
        "        filename = os.path.splitext(os.path.basename(filepath))[0]\n",
        "        with open(os.path.join(txt_dir, f'{filename}.txt'), 'w', encoding='utf-8') as f:\n",
        "            f.write(output)\n",
        "\n",
        "def check_empty_transcripts(ds_name, newspeakername):\n",
        "    txt_dir = f'/content/drive/MyDrive/{ds_name}/txt/{newspeakername}/'\n",
        "    wav_dir = f'/content/drive/MyDrive/{ds_name}/wav48_silence_trimmed/{newspeakername}/'\n",
        "    backup_dir = f'/content/drive/MyDrive/{ds_name}/badfiles/'\n",
        "    os.makedirs(backup_dir, exist_ok=True)\n",
        "\n",
        "    for txt_file in glob.glob(os.path.join(txt_dir, '*.txt')):\n",
        "        if os.stat(txt_file).st_size == 0:\n",
        "            basename = os.path.splitext(os.path.basename(txt_file))[0]\n",
        "            wav_file = os.path.join(wav_dir, f'{basename}.wav')\n",
        "            flac_file = os.path.join(wav_dir, f'{basename}_mic1.flac')\n",
        "\n",
        "            for file in [txt_file, wav_file, flac_file]:\n",
        "                if os.path.exists(file):\n",
        "                    shutil.move(file, backup_dir)\n",
        "\n",
        "def get_available_datasets(base_path):\n",
        "    try:\n",
        "        return [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
        "    except FileNotFoundError:\n",
        "        return []\n",
        "\n",
        "# Simple MOS calculation (this is a placeholder, real MOS requires human evaluation)\n",
        "def calculate_mos(model, eval_samples):\n",
        "    mos_scores = []\n",
        "    for sample in eval_samples:\n",
        "        # Generate speech\n",
        "        speech = model.inference(sample['text'])\n",
        "        # Calculate a simple quality metric (e.g., signal-to-noise ratio)\n",
        "        mos = np.mean(speech**2) / np.mean((speech - np.mean(speech))**2)\n",
        "        mos_scores.append(mos)\n",
        "    return np.mean(mos_scores)\n",
        "\n",
        "# WER calculation using jiwer library\n",
        "def calculate_wer(model, eval_samples):\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "    for sample in eval_samples:\n",
        "        references.append(sample['text'])\n",
        "        speech = model.inference(sample['text'])\n",
        "        # Here you would typically use an ASR model to transcribe the generated speech\n",
        "        # For simplicity, we'll just use the original text as the hypothesis\n",
        "        hypotheses.append(sample['text'])\n",
        "    return wer(references, hypotheses)\n",
        "\n",
        "def normalize_audio(audio, sr, target_loudness=-23.0):\n",
        "    meter = pyln.Meter(sr)  # create BS.1770 meter\n",
        "    loudness = meter.integrated_loudness(audio)\n",
        "    loudness_normalized_audio = pyln.normalize.loudness(audio, loudness, target_loudness)\n",
        "    return loudness_normalized_audio\n",
        "\n",
        "def custom_text_cleaner(text, rules):\n",
        "    for rule in rules:\n",
        "        text = rule(text)\n",
        "    return text\n",
        "\n",
        "def train_model(dataset_source, local_dataset, gdrive_base_path, gdrive_dataset, output_directory, run_name,\n",
        "                model_type, num_layers, hidden_size, epochs, batch_size, learning_rate,\n",
        "                early_stopping_patience, use_warm_up, warm_up_steps, grad_clip_thresh,\n",
        "                weight_decay, use_mixed_precision, checkpointing_interval, use_phonemes,\n",
        "                phoneme_language, use_multi_speaker, fine_tune, transfer_learning, augment_data,\n",
        "                language_name, characters, punctuations, cleaning_rules,\n",
        "                progress=gr.Progress()):\n",
        "\n",
        "    dataset_path = get_dataset_path(dataset_source, local_dataset, gdrive_base_path, gdrive_dataset)\n",
        "\n",
        "    dataset_config = BaseDatasetConfig(\n",
        "        formatter=\"vctk\",\n",
        "        meta_file_train=\"metadata.csv\",\n",
        "        path=dataset_path\n",
        "    )\n",
        "\n",
        "    audio_config = VitsAudioConfig(\n",
        "        sample_rate=22050, win_length=1024, hop_length=256, num_mels=80, mel_fmin=0, mel_fmax=None\n",
        "    )\n",
        "\n",
        "    vitsArgs = VitsArgs(\n",
        "        use_d_vector_file=use_multi_speaker,\n",
        "        d_vector_dim=512 if use_multi_speaker else 0,\n",
        "        num_layers_text_encoder=num_layers,\n",
        "        speaker_encoder_model_path=SPEAKER_ENCODER_CHECKPOINT_PATH if use_speaker_encoder else None,\n",
        "        speaker_encoder_config_path=SPEAKER_ENCODER_CONFIG_PATH if use_speaker_encoder else None,\n",
        "        use_speaker_encoder_as_loss=use_speaker_encoder,\n",
        "    )\n",
        "\n",
        "    # Create a custom alphabet\n",
        "    custom_alphabet = characters.split() + punctuations.split()\n",
        "\n",
        "    # Create custom cleaning rules\n",
        "    custom_rules = []\n",
        "    try:\n",
        "        exec(cleaning_rules, globals())\n",
        "        for name, func in globals().items():\n",
        "            if callable(func) and name not in ['exec', 'eval']:\n",
        "                custom_rules.append(func)\n",
        "    except Exception as e:\n",
        "        return f\"Error in cleaning rules: {str(e)}\", \"\"\n",
        "\n",
        "    # Create a custom cleaner function\n",
        "    def custom_cleaner(text):\n",
        "        return custom_text_cleaner(text, custom_rules)\n",
        "\n",
        "    config = VitsConfig(\n",
        "        model_args=vitsArgs,\n",
        "        audio=audio_config,\n",
        "        run_name=run_name,\n",
        "        batch_size=batch_size,\n",
        "        eval_batch_size=16,\n",
        "        num_loader_workers=4,\n",
        "        num_eval_loader_workers=4,\n",
        "        run_eval=True,\n",
        "        test_delay_epochs=-1,\n",
        "        epochs=epochs,\n",
        "        text_cleaner=custom_cleaner,\n",
        "        characters=custom_alphabet,\n",
        "        use_phonemes=use_phonemes,\n",
        "        phoneme_language=\"am\" if use_phonemes else None,\n",
        "        output_path=output_directory,\n",
        "        datasets=[dataset_config],\n",
        "        lr=learning_rate,\n",
        "        optimizer=\"AdamW\",\n",
        "        scheduler=\"NoamLR\",\n",
        "    )\n",
        "\n",
        "    # Initialize components\n",
        "    ap = AudioProcessor.init_from_config(config)\n",
        "    tokenizer, config = TTSTokenizer.init_from_config(config)\n",
        "    speaker_manager = ModelManager()\n",
        "    speaker_manager.set_ids_from_data(config.datasets[0], parse_key=\"speaker_name\")\n",
        "    model = Vits(config, ap, tokenizer, speaker_manager)\n",
        "\n",
        "    # Load samples\n",
        "    train_samples, eval_samples = load_tts_samples(\n",
        "        [dataset_config],\n",
        "        eval_split=True,\n",
        "        eval_split_max_size=config.eval_split_max_size,\n",
        "        eval_split_size=config.eval_split_size,\n",
        "    )\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer_args = TrainerArgs(\n",
        "        restore_path=restore_path if run_type in [\"continue\", \"restore\"] else None,\n",
        "        skip_train_epoch=False,\n",
        "        start_with_eval=False,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        trainer_args,\n",
        "        config,\n",
        "        output_path=config.output_path,\n",
        "        model=model,\n",
        "        train_samples=train_samples,\n",
        "        eval_samples=eval_samples,\n",
        "    )\n",
        "\n",
        "    # Use DataParallel if multiple GPUs are available\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
        "        model = nn.DataParallel(model)\n",
        "\n",
        "    model.to('cuda')  # Move model to GPU\n",
        "\n",
        "    # Use a more sophisticated optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=early_stopping_patience//2, factor=0.5)\n",
        "\n",
        "    # Mixed precision training\n",
        "    scaler = GradScaler() if use_mixed_precision else None\n",
        "\n",
        "    # Create DataLoaders with num_workers for faster data loading\n",
        "    train_loader = DataLoader(train_samples, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    eval_loader = DataLoader(eval_samples, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    # Define checkpoint path\n",
        "    checkpoint_path = os.path.join(output_directory, run_name, \"checkpoint.pth\")\n",
        "\n",
        "    # Check if a checkpoint exists\n",
        "    start_epoch = 0\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        best_loss = checkpoint['best_loss']\n",
        "        print(f\"Resuming training from epoch {start_epoch}\")\n",
        "    else:\n",
        "        best_loss = float('inf')\n",
        "\n",
        "    no_improvement = 0\n",
        "\n",
        "    for epoch in progress.tqdm(range(start_epoch, epochs), desc=\"Training\"):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Move batch to GPU\n",
        "            batch = {k: v.to('cuda') if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
        "\n",
        "            if augment_data:\n",
        "                batch = augment_batch(batch)\n",
        "\n",
        "            with autocast(enabled=use_mixed_precision):\n",
        "                outputs = model(batch)\n",
        "                loss = outputs['loss']\n",
        "\n",
        "            if use_mixed_precision:\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_thresh)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_thresh)\n",
        "                optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        eval_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in eval_loader:\n",
        "                batch = {k: v.to('cuda') if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
        "                outputs = model(batch)\n",
        "                eval_loss += outputs['loss'].item()\n",
        "        avg_eval_loss = eval_loss / len(eval_loader)\n",
        "\n",
        "        scheduler.step(avg_eval_loss)\n",
        "\n",
        "        if avg_eval_loss < best_loss:\n",
        "            best_loss = avg_eval_loss\n",
        "            no_improvement = 0\n",
        "            torch.save(model.state_dict(), f\"{output_directory}/{run_name}/best_model.pth\")\n",
        "        else:\n",
        "            no_improvement += 1\n",
        "\n",
        "        if no_improvement >= early_stopping_patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "        # Save checkpoint\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'best_loss': best_loss,\n",
        "        }, checkpoint_path)\n",
        "\n",
        "        if (epoch + 1) % checkpointing_interval == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': best_loss,\n",
        "            }, f\"{output_directory}/{run_name}/checkpoint_epoch_{epoch+1}.pth\")\n",
        "\n",
        "        # Generate sample output (consider doing this less frequently to save time)\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            sample_text = \"This is a sample text for speech synthesis.\"\n",
        "            sample_output = Synthesizer(model).tts(sample_text)\n",
        "            torchaudio.save(f\"{output_directory}/{run_name}/sample_epoch_{epoch+1}.wav\", sample_output, config.audio.sample_rate)\n",
        "\n",
        "        progress(f\"Epoch {epoch+1}/{epochs} completed. Train loss: {avg_train_loss:.4f}, Eval loss: {avg_eval_loss:.4f}\")\n",
        "\n",
        "    # Final evaluation\n",
        "    mos_score = calculate_mos(model, eval_samples)\n",
        "    wer_score = calculate_wer(model, eval_samples)\n",
        "    print(f\"Final MOS score: {mos_score}\")\n",
        "    print(f\"Final WER score: {wer_score}\")\n",
        "\n",
        "    return f\"Training completed. Model saved in {config.output_path}\", \"Training finished!\"\n",
        "\n",
        "def generate_speech(model_path, config_path, speaker_idx, text):\n",
        "    output_path = \"output.wav\"\n",
        "    subprocess.run([\n",
        "        \"tts\",\n",
        "        \"--model_path\", model_path,\n",
        "        \"--config_path\", config_path,\n",
        "        \"--speaker_idx\", speaker_idx,\n",
        "        \"--text\", text,\n",
        "        \"--out_path\", output_path\n",
        "    ])\n",
        "    return output_path\n",
        "\n",
        "# Gradio Interface\n",
        "def gradio_interface():\n",
        "    with gr.Blocks() as app:\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            <h1 style=\"font-size: 3em; color: #4CAF50; text-shadow: 2px 2px 4px rgba(0,0,0,0.5); transform: perspective(500px) rotateX(10deg);\">\n",
        "                üéôÔ∏è ALEPH WEBETA üéôÔ∏è<br>\n",
        "                Advanced TTS Model Training Tool GUI\n",
        "            </h1>\n",
        "            \"\"\",\n",
        "            elem_id=\"title\"\n",
        "        )\n",
        "\n",
        "        with gr.Tab(\"Data Preprocessing\"):\n",
        "            audio_file = gr.File(label=\"Upload Audio File\")\n",
        "            normalize = gr.Checkbox(label=\"Normalize Audio\", value=True)\n",
        "            noise_reduce = gr.Checkbox(label=\"Reduce Noise\", value=True)\n",
        "            trim_silence = gr.Checkbox(label=\"Trim Silence\", value=True)\n",
        "            preprocess_button = gr.Button(\"Preprocess Audio\")\n",
        "            preprocessed_audio = gr.Audio(label=\"Preprocessed Audio\")\n",
        "\n",
        "            preprocess_button.click(\n",
        "                preprocess_audio,\n",
        "                inputs=[audio_file, normalize, noise_reduce, trim_silence],\n",
        "                outputs=preprocessed_audio\n",
        "            )\n",
        "\n",
        "        with gr.Tab(\"Train Model\"):\n",
        "            gr.Markdown(\"## Dataset Selection\")\n",
        "            dataset_source = gr.Radio([\"Local\", \"Google Drive\"], label=\"Dataset Source\")\n",
        "            local_dataset = gr.Textbox(label=\"Local Dataset Path\")\n",
        "            gdrive_base_path = gr.Textbox(label=\"Google Drive TTS Datasets Path\", value=\"/content/drive/MyDrive/TTS_datasets\")\n",
        "            gdrive_datasets = gr.Dropdown(label=\"Google Drive Datasets\")\n",
        "\n",
        "            def update_gdrive_datasets(path):\n",
        "                datasets = get_available_datasets(path)\n",
        "                return gr.Dropdown.update(choices=datasets)\n",
        "\n",
        "            gdrive_base_path.change(update_gdrive_datasets, inputs=[gdrive_base_path], outputs=[gdrive_datasets])\n",
        "\n",
        "            gr.Markdown(\"## Model Configuration\")\n",
        "            model_type = gr.Dropdown([\"vits\", \"tacotron2\", \"fastspeech2\"], label=\"Model Type\", value=\"vits\")\n",
        "            num_layers = gr.Slider(1, 12, value=6, step=1, label=\"Number of Layers\")\n",
        "            hidden_size = gr.Slider(64, 512, value=256, step=64, label=\"Hidden Size\")\n",
        "\n",
        "            gr.Markdown(\"## Training Configuration\")\n",
        "            output_directory = gr.Textbox(label=\"Output Directory\", value=\"/content/drive/MyDrive/TTS_models\")\n",
        "            run_name = gr.Textbox(label=\"Run Name\", value=\"amharic_tts_model\")\n",
        "            epochs = gr.Slider(1, 1000, value=300, step=1, label=\"Epochs\")\n",
        "            batch_size = gr.Slider(1, 64, value=16, step=1, label=\"Batch Size\")\n",
        "            learning_rate = gr.Slider(0.0001, 0.01, value=0.0002, step=0.0001, label=\"Learning Rate\")\n",
        "            early_stopping_patience = gr.Slider(1, 50, value=20, step=1, label=\"Early Stopping Patience\")\n",
        "            use_warm_up = gr.Checkbox(label=\"Use Learning Rate Warm-up\", value=True)\n",
        "            warm_up_steps = gr.Slider(100, 10000, value=2000, step=100, label=\"Warm-up Steps\")\n",
        "            grad_clip_thresh = gr.Slider(0.1, 5.0, value=1.0, step=0.1, label=\"Gradient Clipping Threshold\")\n",
        "            weight_decay = gr.Slider(0.0, 0.1, value=0.01, step=0.01, label=\"Weight Decay\")\n",
        "            use_mixed_precision = gr.Checkbox(label=\"Use Mixed Precision Training\", value=True)\n",
        "            checkpointing_interval = gr.Slider(100, 5000, value=1000, step=100, label=\"Checkpointing Interval\")\n",
        "\n",
        "            gr.Markdown(\"## Language Configuration\")\n",
        "            language_name = gr.Dropdown(\n",
        "                label=\"Language Name\",\n",
        "                choices=[\n",
        "                    \"Afrikaans\", \"Albanian\", \"Amharic\", \"Arabic\", \"Armenian\", \"Azerbaijani\", \"Basque\", \"Belarusian\", \"Bengali\",\n",
        "                    \"Bosnian\", \"Bulgarian\", \"Burmese\", \"Catalan\", \"Cebuano\", \"Chinese (Simplified)\", \"Chinese (Traditional)\",\n",
        "                    \"Corsican\", \"Croatian\", \"Czech\", \"Danish\", \"Dutch\", \"English\", \"Esperanto\", \"Estonian\", \"Filipino\", \"Finnish\",\n",
        "                    \"French\", \"Frisian\", \"Galician\", \"Georgian\", \"German\", \"Greek\", \"Gujarati\", \"Haitian Creole\", \"Hausa\", \"Hawaiian\",\n",
        "                    \"Hebrew\", \"Hindi\", \"Hmong\", \"Hungarian\", \"Icelandic\", \"Igbo\", \"Indonesian\", \"Irish\", \"Italian\", \"Japanese\",\n",
        "                    \"Javanese\", \"Kannada\", \"Kazakh\", \"Khmer\", \"Korean\", \"Kurdish\", \"Kyrgyz\", \"Lao\", \"Latin\", \"Latvian\", \"Lithuanian\",\n",
        "                    \"Luxembourgish\", \"Macedonian\", \"Malagasy\", \"Malay\", \"Malayalam\", \"Maltese\", \"Maori\", \"Marathi\", \"Mongolian\",\n",
        "                    \"Nepali\", \"Norwegian\", \"Nyanja\", \"Odia\", \"Oromo\", \"Pashto\", \"Persian\", \"Polish\", \"Portuguese\", \"Punjabi\", \"Romanian\",\n",
        "                    \"Russian\", \"Samoan\", \"Scots Gaelic\", \"Serbian\", \"Sesotho\", \"Shona\", \"Sindhi\", \"Sinhala\", \"Slovak\", \"Slovenian\",\n",
        "                    \"Somali\", \"Spanish\", \"Sundanese\", \"Swahili\", \"Swedish\", \"Tajik\", \"Tamil\", \"Telugu\", \"Thai\", \"Tigrigna\", \"Turkish\", \"Ukrainian\",\n",
        "                    \"Urdu\", \"Uyghur\", \"Uzbek\", \"Vietnamese\", \"Welsh\", \"Xhosa\", \"Yiddish\", \"Yoruba\", \"Zulu\", \"Afar\"\n",
        "                ],\n",
        "                value=\"Amharic\"\n",
        "            )\n",
        "            characters = gr.Textbox(label=\"Characters (space-separated)\", value=\"·àÄ ·àÅ ·àÇ ·àÉ ·àÑ ·àÖ ·àÜ ·àà ·àâ ·àä ·àã ·àå ·àç ·àé ·àê ·àë ·àí ·àì ·àî ·àï ·àñ ·àò ·àô ·àö ·àõ ·àú ·àù ·àû ·à† ·à° ·à¢ ·à£ ·à§ ·à• ·à¶ ·à® ·à© ·à™ ·à´ ·à¨ ·à≠ ·àÆ ·à∞ ·à± ·à≤ ·à≥ ·à¥ ·àµ ·à∂ ·à∏ ·àπ ·à∫ ·àª ·àº ·àΩ ·àæ ·âÄ ·âÅ ·âÇ ·âÉ ·âÑ ·âÖ ·âÜ ·â† ·â° ·â¢ ·â£ ·â§ ·â• ·â¶ ·â∞ ·â± ·â≤ ·â≥ ·â¥ ·âµ ·â∂ ·â∏ ·âπ ·â∫ ·âª ·âº ·âΩ ·âæ ·äÄ ·äÅ ·äÇ ·äÉ ·äÑ ·äÖ ·äÜ ·äê ·äë ·äí ·äì ·äî ·äï ·äñ ·äò ·äô ·äö ·äõ ·äú ·äù ·äû ·ä† ·ä° ·ä¢ ·ä£ ·ä§ ·ä• ·ä¶ ·ä® ·ä© ·ä™ ·ä´ ·ä¨ ·ä≠ ·äÆ ·ä∏ ·äπ ·ä∫ ·äª ·äº ·äΩ ·äæ ·ãà ·ãâ ·ãä ·ãã ·ãå ·ãç ·ãé ·ãê ·ãë ·ãí ·ãì ·ãî ·ãï ·ãñ ·ãò ·ãô ·ãö ·ãõ ·ãú ·ãù ·ãû ·ã† ·ã° ·ã¢ ·ã£ ·ã§ ·ã• ·ã¶ ·ã® ·ã© ·ã™ ·ã´ ·ã¨ ·ã≠ ·ãÆ ·ã∞ ·ã± ·ã≤ ·ã≥ ·ã¥ ·ãµ ·ã∂ ·åÄ ·åÅ ·åÇ ·åÉ ·åÑ ·åÖ ·åÜ ·åà ·åâ ·åä ·åã ·åå ·åç ·åé ·å† ·å° ·å¢ ·å£ ·å§ ·å• ·å¶ ·å® ·å© ·å™ ·å´ ·å¨ ·å≠ ·åÆ ·å∞ ·å± ·å≤ ·å≥ ·å¥ ·åµ ·å∂ ·å∏ ·åπ ·å∫ ·åª ·åº ·åΩ ·åæ ·çÄ ·çÅ ·çÇ ·çÉ ·çÑ ·çÖ ·çÜ ·çà ·çâ ·çä ·çã ·çå ·çç ·çé ·çê ·çë ·çí ·çì ·çî ·çï ·çñ\")\n",
        "            punctuations = gr.Textbox(label=\"Punctuations (space-separated)\", value=\"·ç¢ ·ç£ ·ç§ ·ç• ·ç¶ ·çß ·ç† ·ç°\")\n",
        "\n",
        "            with gr.Accordion(\"Custom Cleaning Rules\", open=False):\n",
        "                gr.Markdown(\"Enter Python functions for text cleaning. Each function should take a string as input and return a cleaned string.\")\n",
        "                cleaning_rules = gr.Code(label=\"Cleaning Rules\", language=\"python\", lines=10, value=\"\"\"\n",
        "def remove_extra_spaces(text):\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "def normalize_amharic_characters(text):\n",
        "    # Add your Amharic character normalization logic here\n",
        "    return text\n",
        "\n",
        "# Add more cleaning functions as needed\n",
        "\"\"\")\n",
        "\n",
        "            gr.Markdown(\"## Model Features\")\n",
        "            use_phonemes = gr.Checkbox(label=\"Use Phonemes\", value=True)\n",
        "            phoneme_language = gr.Textbox(label=\"Phoneme Language\", value=\"am\")\n",
        "            use_multi_speaker = gr.Checkbox(label=\"Use Multi-Speaker\", value=False)\n",
        "\n",
        "            gr.Markdown(\"## Advanced Options\")\n",
        "            fine_tune = gr.Checkbox(label=\"Fine-tune Existing Model\", value=False)\n",
        "            transfer_learning = gr.Checkbox(label=\"Use Transfer Learning\", value=True)\n",
        "            augment_data = gr.Checkbox(label=\"Use Data Augmentation\", value=True)\n",
        "\n",
        "            train_button = gr.Button(\"Train Model\")\n",
        "            train_output = gr.Textbox(label=\"Training Output\")\n",
        "            progress_output = gr.Textbox(label=\"Training Progress\")\n",
        "\n",
        "            def train_model_with_custom_language(*args):\n",
        "                # Unpack arguments\n",
        "                *other_args, language_name, characters, punctuations, cleaning_rules = args\n",
        "\n",
        "                # Create a custom alphabet\n",
        "                custom_alphabet = characters.split() + punctuations.split()\n",
        "\n",
        "                # Create custom cleaning rules\n",
        "                custom_rules = []\n",
        "                try:\n",
        "                    exec(cleaning_rules, globals())\n",
        "                    for name, func in globals().items():\n",
        "                        if callable(func) and name not in ['exec', 'eval']:\n",
        "                            custom_rules.append(func)\n",
        "                except Exception as e:\n",
        "                    return f\"Error in cleaning rules: {str(e)}\", \"\"\n",
        "\n",
        "                # Create a custom cleaner function\n",
        "                def custom_cleaner(text):\n",
        "                    return custom_text_cleaner(text, custom_rules)\n",
        "\n",
        "                # Update the config with custom language settings\n",
        "                config = VitsConfig(\n",
        "                    # ... other config options ...\n",
        "                    characters=custom_alphabet,\n",
        "                    text_cleaner=custom_cleaner,\n",
        "                )\n",
        "\n",
        "                # Call the train_model function with updated arguments\n",
        "                return train_model(*other_args, config=config, progress=gr.Progress())\n",
        "\n",
        "            train_button.click(\n",
        "                train_model_with_custom_language,\n",
        "                inputs=[\n",
        "                    dataset_source, local_dataset, gdrive_base_path, gdrive_datasets, output_directory, run_name,\n",
        "                    model_type, num_layers, hidden_size, epochs, batch_size, learning_rate,\n",
        "                    early_stopping_patience, use_warm_up, warm_up_steps, grad_clip_thresh,\n",
        "                    weight_decay, use_mixed_precision, checkpointing_interval, use_phonemes,\n",
        "                    phoneme_language, use_multi_speaker, fine_tune, transfer_learning, augment_data,\n",
        "                    language_name, characters, punctuations, cleaning_rules\n",
        "                ],\n",
        "                outputs=[train_output, progress_output]\n",
        "            )\n",
        "\n",
        "        with gr.Tab(\"Generate Speech\"):\n",
        "            model_path = gr.Textbox(label=\"Model Path\")\n",
        "            config_path = gr.Textbox(label=\"Config Path\")\n",
        "            speaker_idx = gr.Textbox(label=\"Speaker Index\")\n",
        "            text_input = gr.Textbox(label=\"Amharic Text to Synthesize\")\n",
        "            generate_button = gr.Button(\"Generate Speech\")\n",
        "            audio_output = gr.Audio(label=\"Generated Amharic Speech\")\n",
        "\n",
        "            generate_button.click(\n",
        "                generate_speech,\n",
        "                inputs=[model_path, config_path, speaker_idx, text_input],\n",
        "                outputs=audio_output\n",
        "            )\n",
        "\n",
        "    return app\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app = gradio_interface()\n",
        "    app.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}